{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjVWLnqHxv0K"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets --quiet\n",
        "!pip install accelerate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbformat"
      ],
      "metadata": {
        "id": "MceB4I4sLdit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "H12mMBN917Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "VVOGpdgGSJWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename(\"train.jsonl.txt\", \"train.jsonl\")  # or whatever your file is called"
      ],
      "metadata": {
        "id": "_fsTBKYCzIgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Show all files in your current workspace\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "ylQw7YEizPtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"r\") as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]  # removes empty lines\n",
        "\n",
        "try:\n",
        "    data = [json.loads(line) for line in lines]\n",
        "    print(\"First entry:\", data[0])\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Error in JSON format:\", e)"
      ],
      "metadata": {
        "id": "tjy10Y558JEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "bYISu2_8S0hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"r\") as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]  # skip blank lines\n",
        "    data = [json.loads(line) for line in lines]           # parse each line as JSON\n",
        "\n",
        "print(\"✅ First entry:\")\n",
        "print(data[0])"
      ],
      "metadata": {
        "id": "nAuUMaUGc53O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "cleaned_data = []\n",
        "\n",
        "with open(\"train.jsonl\", \"r\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # skip blank lines\n",
        "        try:\n",
        "            cleaned_data.append(json.loads(line))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Skipping line {i}: JSON decode error → {e}\")\n",
        "\n",
        "# Test: Show first entry\n",
        "if cleaned_data:\n",
        "    print(\"✅ First valid entry:\")\n",
        "    print(cleaned_data[0])\n",
        "else:\n",
        "    print(\"❌ No valid entries found.\")"
      ],
      "metadata": {
        "id": "2wZniE4XdzN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "wT4wbRij2RvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "C5s4mIhRGXUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list([\n",
        "    {\"text\": f\"### Question:\\n{item.get('prompt', '')}\\n\\n### Answer:\\n{item.get('completion', '')}\"}\n",
        "    for item in data\n",
        "])\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "wyMAvhyCC8Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].keys())\n",
        "print(data[1].keys())"
      ],
      "metadata": {
        "id": "hHvzwss4ezGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list([\n",
        "    {\"text\": f\"### Question:\\n{item.get('prompt', '')}\\n\\n### Answer:\\n{item.get('completion', '')}\"}\n",
        "    for item in data\n",
        "])\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "PgwWv1gmHhXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"r\") as f:\n",
        "    data = []\n",
        "    for line in f:\n",
        "        line = line.strip()  # Remove whitespace\n",
        "        if line:  # Skip empty lines\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding line: {line}\")\n",
        "                raise e"
      ],
      "metadata": {
        "id": "NTngSUOELuz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "fefSDL94H-Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.rename(\"train.jsonl.txt\", \"train.jsonl\")"
      ],
      "metadata": {
        "id": "m1p2X1PIIKTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "data = [json.loads(line) for line in lines]\n",
        "print(f\"Loaded {len(data)} entries.\")\n",
        "print(\"Here is a sample entry:\", data[0])"
      ],
      "metadata": {
        "id": "qktaQ5lKISYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "new_entries = [\n",
        "    {\"prompt\": \"How do I create my first budget?\", \"completion\": \"Start by tracking...\"},\n",
        "    # ... [all 50 entries] ...\n",
        "]\n",
        "\n",
        "# Append to dataset\n",
        "with open(\"train.jsonl\", \"a\") as f:\n",
        "    for entry in new_entries:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "# Update data variable to include new entries\n",
        "data += new_entries  # Now contains original + new data\n",
        "print(f\"Added {len(new_entries)} new entries. Total: {len(data)}\")\n",
        "\n",
        "# [Continue with your existing code...]\n",
        "dataset = Dataset.from_list([\n",
        "    {\"text\": f\"### Question:\\n{item['prompt']}\\n\\n### Answer:\\n{item['completion']}\"}\n",
        "    for item in data  # Will now include all data\n",
        "])"
      ],
      "metadata": {
        "id": "uF0BpwUucg0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list([\n",
        "    {\"text\": f\"### Question:\\n{item['prompt']}\\n\\n### Answer:\\n{item['completion']}\"}\n",
        "    for item in data\n",
        "])\n",
        "\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "4543JFpkIcfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].keys())"
      ],
      "metadata": {
        "id": "RPCM9mAAJAIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list([\n",
        "    {\"text\": f\"### Question:\\n{item['prompt']}\\n\\n### Answer:\\n{item['completion']}\"}\n",
        "    for item in data\n",
        "])"
      ],
      "metadata": {
        "id": "va1iHhqAJABF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "uy8JwvorJO1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "os.environ[\"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\"] = \"your_token_here\""
      ],
      "metadata": {
        "id": "ZeCpIwIOKau7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# os.environ[\"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\"] = \"your_personal_hf_token_here\" # Removed for sharing\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "import json"
      ],
      "metadata": {
        "id": "WmgeGhXaK90k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "YlWIET-NLOrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "7FYGtz-VLR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "pvdLJVdlMrNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "os.environ[\"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\"] = \"your_token_here\""
      ],
      "metadata": {
        "id": "sBE3EAzuM0zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-financial-tutor\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[]  # disables logging to wandb or hub\n",
        ")"
      ],
      "metadata": {
        "id": "8JqpyyCuNICd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "P87WE34sP07z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "1c0Dxx-1QA7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "C0lSNvUcT4bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "9EWbMJ-1T6zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "y3zYQhKpUXXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=True,      # pad all sequences to same length\n",
        "        truncation=True,   # cut off sequences longer than max length\n",
        "        max_length=128,    # or whatever max length you want\n",
        "        return_tensors=None  # important to not return PyTorch tensors here\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9YsQfFsCUj6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # because GPT-2 is causal LM, no masked language modeling\n",
        ")"
      ],
      "metadata": {
        "id": "sf2j8JzuUrsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "yLJl3dDIUuWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before: Bad (nested lists)\n",
        "{\"text\": [\"This\", \"is\", \"a\", \"list\"]}\n",
        "\n",
        "# After: Good (flat strings)\n",
        "{\"text\": \"This is a string\"}"
      ],
      "metadata": {
        "id": "TQR2ZaGgMkwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "OlkmCXKbMn4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch  # Only needed if you're using PyTorch\n",
        "\n",
        "# 1. Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set padding token (GPT-2 doesn't have one by default)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Using EOS token for padding\n",
        "\n",
        "# 2. Your input text (modify this with your data)\n",
        "your_text_data = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here's another example for tokenization.\",\n",
        "    \"Short one.\"\n",
        "]\n",
        "\n",
        "# 3. Tokenize the text\n",
        "tokenized_data = tokenizer(\n",
        "    your_text_data,\n",
        "    padding=True,         # Pad to the longest sequence in the batch\n",
        "    truncation=True,      # Truncate to the model's max length (512 for GPT-2)\n",
        "    max_length=512,       # Explicitly set max length (optional)\n",
        "    return_tensors=\"pt\"   # Return PyTorch tensors (\"tf\" for TensorFlow, \"np\" for NumPy)\n",
        ")\n",
        "\n",
        "# 4. Output the results\n",
        "print(\"Tokenized data structure:\", tokenized_data)\n",
        "print(\"\\nInput IDs (token indices):\")\n",
        "print(tokenized_data[\"input_ids\"])\n",
        "print(\"\\nAttention Mask (1=real token, 0=padding):\")\n",
        "print(tokenized_data[\"attention_mask\"])"
      ],
      "metadata": {
        "id": "xjkMwLosMzed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # Directory to save checkpoints\n",
        "    per_device_train_batch_size=4,    # Batch size\n",
        "    num_train_epochs=3,               # Number of epochs\n",
        "    save_steps=10_000,                # Save model every X steps\n",
        "    logging_dir=\"./logs\",             # Logging directory\n",
        "    logging_steps=100,                # Log every X steps\n",
        "    learning_rate=5e-5,               # Learning rate\n",
        "    fp16=True,                        # Use mixed precision (if GPU supports it)\n",
        ")"
      ],
      "metadata": {
        "id": "DAaA5LFZU1fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs this for padding\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Tokenize data (WITH LABELS)\n",
        "texts = [\"Hello world!\", \"GPT-2 is cool.\"]\n",
        "tokenized_data = tokenizer(\n",
        "    texts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"]  # Key fix: Add labels\n",
        "\n",
        "# 3. Create Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "dataset = TextDataset(tokenized_data)\n",
        "\n",
        "# 4. Training args (customize as needed)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_steps=10_000,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,  # Enable if using a GPU\n",
        "    run_name=\"gpt2-finetune\",  # Fixes the wandb warning\n",
        ")\n",
        "\n",
        "# 5. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# 6. Train!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "lVPm6ANhVZUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, encodings):\n",
        "        self.texts = texts  # Store original texts\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['text'] = self.texts[idx]  # Add original text\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "# When creating dataset:\n",
        "texts = [\"Hello world!\", \"GPT-2 is cool.\"]  # Your original texts\n",
        "tokenized_data = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"]"
      ],
      "metadata": {
        "id": "FyTn-_VnWG6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    # examples[\"text\"] is a list of strings here\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )"
      ],
      "metadata": {
        "id": "aHviOP96VJv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, encodings):  # Takes both arguments\n",
        "        self.texts = texts\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['text'] = self.texts[idx]  # Optional: include raw text\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "# Usage:\n",
        "texts = [\"Hello world!\", \"GPT-2 is cool.\"]  # Original texts\n",
        "tokenized_data = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"]\n",
        "\n",
        "dataset = TextDataset(texts, tokenized_data)  # Pass BOTH arguments"
      ],
      "metadata": {
        "id": "U5ClPbc-Wmjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset  # ← Import from Hugging Face datasets library\n",
        "\n",
        "# 1. Convert your data to HF Dataset format\n",
        "raw_texts = [\"Hello world!\", \"GPT-2 is cool.\"]  # Your original texts\n",
        "hf_dataset = Dataset.from_dict({\"text\": raw_texts})\n",
        "\n",
        "# 2. Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# 3. Apply tokenization\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset.set_format(\"torch\")  # Convert to PyTorch tensors\n",
        "\n",
        "# Now use with Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "1JhP0cuCXFsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "o7kXqXUjVrO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    logging_dir=\"./logs\"\n",
        ")"
      ],
      "metadata": {
        "id": "Zs_VqbTZjXAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    # Optional: eval_dataset=tokenized_eval_dataset, if you have validation data\n",
        ")"
      ],
      "metadata": {
        "id": "JORfCaaEjzyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Sample data\n",
        "texts = [\"Hello world!\", \"GPT-2 is cool.\"]\n",
        "\n",
        "# Create HF Dataset\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# Tokenize\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=512),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Add labels (fixed version)\n",
        "tokenized_dataset = tokenized_dataset.map(\n",
        "    lambda x: {\"labels\": x[\"input_ids\"].copy()},\n",
        "    batched=True,\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "# Verify\n",
        "print(tokenized_dataset[0].keys())"
      ],
      "metadata": {
        "id": "zP5555iXcaUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_labels(example):\n",
        "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels)"
      ],
      "metadata": {
        "id": "NdFnm7grmh9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Verify your dataset has:\n",
        "print(tokenized_dataset[0].keys())\n",
        "# Must include: 'input_ids', 'attention_mask', 'labels'\n",
        "\n",
        "# 2. Check tensor format (PyTorch default)\n",
        "print(type(tokenized_dataset[0]['input_ids']))\n",
        "# Should show: torch.Tensor (or tf.Tensor for TF)\n",
        "\n",
        "# 3. Validate GPU availability\n",
        "import torch\n",
        "print(torch.cuda.is_available())  # Should be True for GPU training"
      ],
      "metadata": {
        "id": "eXCid-NLcopy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets transformers torch"
      ],
      "metadata": {
        "id": "bUF0a_9ddlqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2. Prepare data\n",
        "texts = [\"Example 1\", \"Example 2\", \"Example 3\"]\n",
        "\n",
        "# 3. Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# 4. Tokenize with manual tensor conversion\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    # Convert to PyTorch tensors MANUALLY\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(tokenized[\"input_ids\"]),\n",
        "        \"attention_mask\": torch.tensor(tokenized[\"attention_mask\"]),\n",
        "        \"labels\": torch.tensor(tokenized[\"input_ids\"])  # For LM\n",
        "    }\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=len(texts)  # Process all at once to avoid Arrow conversion\n",
        ")\n",
        "\n",
        "# 5. Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# 6. Train!\n",
        "trainer.train()  # This will now work"
      ],
      "metadata": {
        "id": "eOjzWqjyctar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    # Add labels equal to input_ids for causal LM loss calculation\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "FZsgDe_Mm_BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0].keys())"
      ],
      "metadata": {
        "id": "SE-XqgJPnCKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "e4vlD1zcnE4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "nMbFrphAnHe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('./gpt2-finetuned'))"
      ],
      "metadata": {
        "id": "w4RH4_EjoFBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([d for d in os.listdir(\"gpt2-finetuned\") if d.startswith('checkpoint')])"
      ],
      "metadata": {
        "id": "QCAk7y2XUA62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R gpt2-finetuned"
      ],
      "metadata": {
        "id": "BNHxQurwULNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Use the directory where you saved the model and tokenizer\n",
        "model_path = \"./gpt2-finetuned\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "# Save the model and tokenizer again to make sure all files are present\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Load the model and tokenizer explicitly\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Create the pipeline using the loaded model and tokenizer\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        ")"
      ],
      "metadata": {
        "id": "i2mkpUupfsb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoints = [d for d in os.listdir(\"./gpt2-finetuned\") if d.startswith(\"checkpoint\")]\n",
        "print(\"Available checkpoints:\", checkpoints)"
      ],
      "metadata": {
        "id": "XXso4MigfxJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoints:\n",
        "    latest = sorted(checkpoints)[-1]\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=f\"./gpt2-finetuned/{latest}\",\n",
        "        tokenizer=f\"./gpt2-finetuned/{latest}\"\n",
        "    )\n",
        "else:\n",
        "    print(\"No checkpoints found - using main directory\")\n",
        "    generator = pipeline(\"text-generation\", model=\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "DPrQXSjnf4cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check what's in your directory\n",
        "model_dir = \"./gpt2-finetuned\"\n",
        "print(\"Contents of model directory:\", os.listdir(model_dir))\n",
        "\n",
        "# Required files\n",
        "required_files = [\n",
        "    'config.json',\n",
        "    'pytorch_model.bin',  # or model.safetensors\n",
        "    'tokenizer_config.json',\n",
        "    'vocab.json',\n",
        "    'merges.txt'\n",
        "]\n",
        "missing = [f for f in required_files if not os.path.exists(f\"{model_dir}/{f}\")]\n",
        "if missing:\n",
        "    print(f\"❌ Missing files: {missing}\")\n",
        "else:\n",
        "    print(\"✅ All required files present\")"
      ],
      "metadata": {
        "id": "PGnXp4JmgFsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "itMuLRsNgR05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, pipeline\n",
        "\n",
        "# Explicitly load safetensors format\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"./gpt2-finetuned\",\n",
        "    use_safetensors=True  # This is key\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=\"./gpt2-finetuned\"\n",
        ")"
      ],
      "metadata": {
        "id": "lrB7vRXRgzKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# 1. Check files\n",
        "model_dir = \"./gpt2-finetuned\"\n",
        "print(\"Current files:\", os.listdir(model_dir))\n",
        "\n",
        "# 2. Load model (automatic format detection)\n",
        "try:\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "    # 3. Test generation\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "    inputs = tokenizer(\"Explain budgeting:\", return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=100)\n",
        "    print(\"Test generation:\", tokenizer.decode(outputs[0]))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n",
        "    if \"model.safetensors\" in str(e):\n",
        "        print(\"\\n👉 Solution: Add 'use_safetensors=True' to from_pretrained()\")"
      ],
      "metadata": {
        "id": "jRcD4YMeg3VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model_path = \"./gpt2-finetuned\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Try a prompt\n",
        "prompt = \"Explain the basics of finance:\"\n",
        "outputs = generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Print generated text\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "0WAiYOcKpa0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    save_steps=100,  # Save every 100 steps\n",
        "    save_total_limit=3,  # Keep 3 checkpoints\n",
        "    # ... other args ...\n",
        ")"
      ],
      "metadata": {
        "id": "0hNr4EcdUruM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# 1. Load your trained model (or start fresh if needed)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # Change this if you have a custom model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Save with ALL required files\n",
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "# 3. Verify the saved files\n",
        "print(\"Saved files:\", os.listdir(\"./gpt2-finetuned\"))"
      ],
      "metadata": {
        "id": "jL5-xUi3VCNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "\n",
        "# Load from your directory (will automatically find model.safetensors)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Test it\n",
        "output = generator(\"Explain budgeting to a college student:\", max_length=150)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "okvO_M7zVYa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Explain the basic principles of finance, including budgeting, saving, investing, \"\n",
        "    \"and managing debt. Provide a clear overview for someone new to finance.\"\n",
        ")\n",
        "output = generator(prompt, max_new_tokens=150, temperature=0.7, top_p=0.9, truncation=True)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "k_HlxsSXv_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Write a detailed paragraph explaining the basic principles of finance, \"\n",
        "    \"including budgeting, saving, investing, and managing debt. Use simple language \"\n",
        "    \"that is easy for beginners to understand.\"\n",
        ")\n",
        "\n",
        "output = generator(prompt, max_new_tokens=180, temperature=0.7, top_p=0.9, truncation=True)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "4vTps93UwKda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBe6NAl45QBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = outputs[0]['generated_text']\n",
        "# Remove the prompt part to get only the answer\n",
        "answer = generated_text[len(prompt):].strip()\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "scES1SZ4zI5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "Kr0EJpzY3Orm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "Koeg3HVX3F-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./gpt2-finetuned\"  # or \"./gpt2-financial-tutor\" based on the check\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "o_wVE7-b33MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_path = \"./gpt2-finetuned\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1  # CPU; use device=0 if you want GPU\n",
        ")\n",
        "\n",
        "prompt = (\n",
        "    \"### Question:\\n\"\n",
        "    \"Explain the basic principles of finance, including budgeting, saving, investing, and managing debt.\\n\\n### Answer:\\n\"\n",
        ")\n",
        "\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "HIkfb0Em4fYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('./gpt2-finetuned'))"
      ],
      "metadata": {
        "id": "jOMf2Wn04oiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the base GPT-2 tokenizer (or whichever tokenizer you used)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "usEN2VjT5XDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "EKyX_cdx5Z4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_path = \"./gpt2-finetuned\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "1AdcqyxR5b9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "Ho6GBGQ25kHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "OfvgqVZz5ugD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "3KWD2iz753aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
      ],
      "metadata": {
        "id": "_wWzgAaM6BiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "\n",
        "model_path = \"./gpt2-finetuned\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Create a generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1  # CPU, change to 0 for GPU if available\n",
        ")\n",
        "\n",
        "# Your prompt to test the model\n",
        "prompt = (\n",
        "    \"### Question:\\n\"\n",
        "    \"explain what credit is.\\n\\n### Answer:\\n\"\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "p767gzHu6YxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,  # Increase this number\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "eb05Grns7hBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature=1.0,  # More creative and likely to generate longer text\n",
        "top_p=0.95,"
      ],
      "metadata": {
        "id": "S8icOnze7nJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "_FPIkFdX7qSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain the basic principles of finance, including budgeting, saving, investing, and managing debt.\\nAnswer:\"\n",
        "\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "GudIzpdB8nAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"You're a friendly financial literacy tutor. Explain what a credit score is to a teenager with no background in finance..\\nAnswer:\"\n",
        "\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "K5oxj8UI85Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=1.0,       # Lower = more focused, less random\n",
        "    top_p=0.9,             # Still allows some sampling\n",
        "    do_sample=False,       # Disables randomness for more consistency\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "nkUmP_8Q_WsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic generation\n",
        "prompt = \"Explain the 50/30/20 budgeting rule:\"\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_length=150,  # Increase for longer responses\n",
        "    temperature=0.7,  # 0.0-1.0 (lower = more focused)\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "faZw-SrYc43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47826c62"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1  # Use -1 for CPU, or 0 for GPU if available\n",
        ")\n",
        "\n",
        "# Your prompt to test the model\n",
        "prompt = \"### Question:\\nWhat is a debit card?\\n\\n### Answer:\\n\"\n",
        "\n",
        "# Generate text\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=150,  # Adjust the number of tokens as needed\n",
        "    temperature=0.5,     # Controls randomness\n",
        "    top_p=0.9,           # Controls diversity\n",
        "    do_sample=True,      # Enable sampling\n",
        "    pad_token_id=tokenizer.eos_token_id # Use EOS token for padding\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}